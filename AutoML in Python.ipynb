{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML with H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "import pandas as pd\n",
    "# start cluster\n",
    "h2o.init()\n",
    "# convert to h2o frame\n",
    "traindf = h2o.H2OFrame(r.train)\n",
    "testdf = h2o.H2OFrame(r.test)\n",
    "y = \"Species\"\n",
    "x = list(traindf.columns)\n",
    "x.remove(y)\n",
    "# create df to factors\n",
    "traindf[y] = traindf[y].asfactor()\n",
    "testdf[y] = testdf[y].asfactor()\n",
    "#run automl\n",
    "aml = H2OAutoML(max_runtime_secs = 60)\n",
    "aml.train(x = x, y = y, training_frame = traindf)\n",
    "# view leader board\n",
    "aml.leaderboard\n",
    "# do pridiction and convert it to a data frame\n",
    "predict = aml.predict(testdf)\n",
    "p = predict.as_data_frame()\n",
    "# convert to pandas dataframe\n",
    "data = {'actual': r.test.Species, 'Ypredict': p['predict'].tolist()}\n",
    "df = pd.DataFrame(data, columns = ['actual','Ypredict'])\n",
    "# create a confusion matrix and print results\n",
    "confusion_matrix = pd.crosstab(df['actual'], df['Ypredict'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print (confusion_matrix)\n",
    "# close h2o connection\n",
    "h2o.shutdown(prompt = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library\n",
    "library(autoxgboost)\n",
    "# create a classification task\n",
    "trainTask = makeClassifTask(data = train, target = \"Species\")\n",
    "# create a control object for optimizer\n",
    "ctrl = makeMBOControl()\n",
    "ctrl = setMBOControlTermination(ctrl, iters = 5L) \n",
    "# fit the model\n",
    "res = autoxgboost(trainTask, control = ctrl, tune.threshold = FALSE)\n",
    "# do prediction and print confusion matrix\n",
    "prediction = predict(res, test[,1:4])\n",
    "caret::confusionMatrix(test$Species, prediction$data$response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autosklearn.classification\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.DataFrame(r.train)\n",
    "test = pd.DataFrame(r.test)\n",
    "\n",
    "x_train = train.iloc[:,1:4]\n",
    "y_train = train[['Species']]\n",
    "print(y_train.head())\n",
    "x_test = test.iloc[:,1:4]\n",
    "y_test = test[['Species']]\n",
    "print(y_test.head())\n",
    "\n",
    "automl = autosklearn.classification.AutoSklearnClassifier()\n",
    "print(\"classifier\")\n",
    "print(\"fittiong\" )\n",
    "automl.fit(x_train, y_train)\n",
    "y_hat = automl.predict(x_test)\n",
    "\n",
    "# convert to pandas dataframe\n",
    "data = {'actual': r.test.Species, 'Ypredict': y_hat.tolist()}\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['actual','Ypredict'])\n",
    "\n",
    "# create a confusion matrix and print results\n",
    "confusion_matrix = pd.crosstab(df['actual'], df['Ypredict'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import autogluon as ag\n",
    "from autogluon import TabularPrediction as task\n",
    "import pandas as pd\n",
    "\n",
    "train_data = task.Dataset(file_path = \"TRAIN_DATA.csv\")\n",
    "test_data = task.Dataset(file_path = \"TEST_DATA.csv\")\n",
    "\n",
    "label_column = 'Species'\n",
    "print(\"Summary of class variable: \\n\", train_data[label_column].describe())\n",
    "\n",
    "predictor = task.fit(train_data = train_data, label = label_column)\n",
    "\n",
    "y_test = test_data[label_column]  # values to predict\n",
    "\n",
    "y_pred = predictor.predict(test_data)\n",
    "print(\"Predictions:  \", y_pred)\n",
    "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n",
    "print(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "print(dataframe.shape)\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "print(X.shape, y.shape)\n",
    "# basic data preparation\n",
    "X = X.astype('float32')\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "# separate into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "# define the search\n",
    "search = StructuredDataClassifier(max_trials=15)\n",
    "# perform the search\n",
    "search.fit(x=X_train, y=y_train, verbose=0)\n",
    "# evaluate the model\n",
    "loss, acc = search.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "# use the model to make a prediction\n",
    "row = [0.0200,0.0371,0.0428,0.0207,0.0954,0.0986,0.1539,0.1601,0.3109,0.2111,0.1609,0.1582,0.2238,0.0645,0.0660,0.2273,0.3100,0.2999,0.5078,0.4797,0.5783,0.5071,0.4328,0.5550,0.6711,0.6415,0.7104,0.8080,0.6791,0.3857,0.1307,0.2604,0.5121,0.7547,0.8537,0.8507,0.6692,0.6097,0.4943,0.2744,0.0510,0.2834,0.2825,0.4256,0.2641,0.1386,0.1051,0.1343,0.0383,0.0324,0.0232,0.0027,0.0065,0.0159,0.0072,0.0167,0.0180,0.0084,0.0090,0.0032]\n",
    "X_new = asarray([row]).astype('float32')\n",
    "yhat = search.predict(X_new)\n",
    "print('Predicted: %.3f' % yhat[0])\n",
    "# get the best performing model\n",
    "model = search.export_model()\n",
    "# summarize the loaded model\n",
    "model.summary()\n",
    "# save the best performing model to file\n",
    "model.save('model_sonar.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# example of hyperopt-sklearn for the sonar classification dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from hpsklearn import HyperoptEstimator\n",
    "from hpsklearn import any_classifier\n",
    "from hpsklearn import any_preprocessing\n",
    "from hyperopt import tpe\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# minimally prepare dataset\n",
    "X = X.astype('float32')\n",
    "y = LabelEncoder().fit_transform(y.astype('str'))\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# define search\n",
    "model = HyperoptEstimator(classifier=any_classifier('cla'), preprocessing=any_preprocessing('pre'), algo=tpe.suggest, max_evals=50, trial_timeout=30)\n",
    "# perform the search\n",
    "model.fit(X_train, y_train)\n",
    "# summarize performance\n",
    "acc = model.score(X_test, y_test)\n",
    "print(\"Accuracy: %.3f\" % acc)\n",
    "# summarize the best model\n",
    "print(model.best_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
